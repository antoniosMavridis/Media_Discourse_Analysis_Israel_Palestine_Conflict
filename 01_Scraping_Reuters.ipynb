{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a222737",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "\n",
    "\n",
    "In this Jupyter Notebook, we embark on a comprehensive exploration of web scraping, with a specific focus on the Reuters news site. We 've developed a function specifically designed to conduct targeted web scraping based on selected keywords. For our study, we chose the keyword \"Israel Hamas\" to focus our efforts on gathering articles related to the Israel-Hamas/Palestine conflict. When this keyword is entered into our function, it triggers a search throughout the Reuters website, systematically retrieving articles that align with this specific subject.  For our project, we established a sample Gmail account, \"advancedcustomeranalytics@gmail.com,\" specifically for the purpose of user creation on the Reuters website. This account facilitated our web scraping process, allowing us to access and collect the required data. Finally, we successfully scraped a total of 2,644 articles from Reuters, which provided us with a substantial dataset for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4dfa3",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a4f144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import json\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740246a",
   "metadata": {},
   "source": [
    "### Initialize Scrapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dac57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuters_search(keyword: str,\n",
    "                   email: str,\n",
    "                   password: str):\n",
    "    \"\"\"\n",
    "    Searches Reuters for articles with the given keyword.\n",
    "    Returns a list of dictionaries with article title, article url,\n",
    "    article date, article source, article text.\n",
    "\n",
    "    Parameters:\n",
    "    - keyword (str): The keyword or search term to search for on Reuters.\n",
    "    - email (str): Your email address for logging into the Reuters website.\n",
    "    - password (str): Your password for logging into the Reuters website.\n",
    "\n",
    "    Returns:\n",
    "    - list of dict: A list of dictionaries containing article information.\n",
    "      Each dictionary includes the following keys: 'title', 'url', 'date', 'source', 'text'.\n",
    "    \"\"\"\n",
    "    \n",
    "    browser = webdriver.Chrome()\n",
    "    login_url = 'https://www.reuters.com/account/sign-in/'\n",
    "    browser.get(login_url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the login elements to be visible and interactable\n",
    "        WebDriverWait(browser, 2).until(EC.visibility_of_element_located((By.NAME, 'email')))\n",
    "        WebDriverWait(browser, 2).until(EC.visibility_of_element_located((By.NAME, 'password')))\n",
    "        \n",
    "        # Accept cookies after signing in\n",
    "        WebDriverWait(browser, 1).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Allow All\")]'))\n",
    "        ).click()\n",
    "\n",
    "        # Input the username and password into the form\n",
    "        browser.find_element(By.NAME, 'email').send_keys(email)\n",
    "        browser.find_element(By.NAME, 'password').send_keys(password)\n",
    "\n",
    "        # Wait for the sign in button with the class 'button__container__3sgvk' to be clickable and click it\n",
    "        sign_in_button = WebDriverWait(browser, 2).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@data-testid=\"Text\"][contains(text(), \"Sign in\")]'))\n",
    "\n",
    "        )\n",
    "        sign_in_button.click()\n",
    "        \n",
    "        # Wait for the URL to change from the current URL, which indicates a redirect\n",
    "        current_url = browser.current_url\n",
    "        WebDriverWait(browser, 2).until(EC.url_changes(current_url))\n",
    "\n",
    "        print(\"Redirection after login successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        browser.quit()\n",
    "        return []\n",
    "\n",
    "  \n",
    "    # Wait for the cookie consent button to be clickable and click it\n",
    "    try:\n",
    "        WebDriverWait(browser, 2).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Allow All\")]'))\n",
    "        ).click()\n",
    "    except:\n",
    "        pass  \n",
    "    \n",
    "    \n",
    "    # After redirection, wait for the search button to be clickable and click it\n",
    "    search_button = WebDriverWait(browser, 2).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//*[@data-testid=\"Button\"][@aria-label=\"Open search bar\"]'))\n",
    "    )\n",
    "    search_button.click()\n",
    "\n",
    "\n",
    "    # Wait for the search input to be present and enter the keyword\n",
    "    search_input = WebDriverWait(browser, 2).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, '//input[@type=\"search\"]'))\n",
    "    )\n",
    "    search_input.send_keys(keyword)\n",
    "    search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "    \n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    content = soup.find('div', attrs = {'class': 'search-results__sectionContainer__34n_c'})\n",
    "    articles = content.find_all('li', attrs = {'class': 'search-results__item__2oqiX'})\n",
    "\n",
    "\n",
    "    article_list = []\n",
    "\n",
    "    while len(article_list) < 3000:\n",
    "        \n",
    "        soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "        # Find all articles on the current page.\n",
    "        articles = soup.find_all('li', attrs={'class': 'search-results__item__2oqiX'})\n",
    "\n",
    "        # Append articles to article_list while checking for duplicates.\n",
    "        for article in articles:\n",
    "            if article not in article_list:\n",
    "                article_list.append(article)\n",
    "\n",
    "        # If we have reached our target number of articles, we can break the loop.\n",
    "        if len(article_list) >= 3000:\n",
    "            break\n",
    "\n",
    "        # Find the next page button and click it, or break if it doesn't exist.\n",
    "        try:\n",
    "            next_button = WebDriverWait(browser, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//button[@data-testid=\"Button\"][contains(@aria-label, \"Next stories\")]'))\n",
    "            )\n",
    "            if next_button:\n",
    "                browser.execute_script(\"arguments[0].click();\", next_button)\n",
    "                \n",
    "                # Wait for the page to load the new content.\n",
    "                WebDriverWait(browser, 5).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//li[@class=\"search-results__item__2oqiX\"]'))\n",
    "                )\n",
    "                time.sleep(2)  \n",
    "              \n",
    "        except Exception as e:\n",
    "            print(f\"No more pages to visit or an error occurred: {e}\")\n",
    "            if len(article_list) > 3000\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            \n",
    "    print(len(article_list))\n",
    "            \n",
    "    for article in article_list:\n",
    "         \n",
    "        # Wait for the cookie consent button to be clickable and click it\n",
    "        try:\n",
    "            WebDriverWait(browser, 2).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Allow All\")]'))\n",
    "            ).click()\n",
    "        except:\n",
    "            pass  \n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            article_dict = {}\n",
    "            a_element = article.find('a')\n",
    "            if not a_element or 'href' not in a_element.attrs:\n",
    "                print(\"continue--------\")\n",
    "                continue\n",
    "\n",
    "            article_link = 'https://www.reuters.com' + a_element['href']\n",
    "            print(article_link)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            browser.get(article_link)\n",
    "            WebDriverWait(browser, 2).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"article\"))\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            article_html = browser.page_source\n",
    "            article_soup = BeautifulSoup(article_html, 'html.parser')\n",
    "\n",
    "            text_container = article_soup.find('div', attrs={'class': 'article-body__content__17Yit'})\n",
    "\n",
    "            article_text = ' '.join(p.get_text() for p in text_container.find_all('p')) if text_container else 'Text not found'\n",
    "\n",
    "            article_dict['title'] = a_element.text.strip()\n",
    "            article_dict['url'] = article_link\n",
    "            article_dict['date'] = article.find('time').text if article.find('time') else 'Date not found'\n",
    "            article_dict['source'] = 'Reuters'\n",
    "            article_dict['text'] = article_text\n",
    "            article_list.append(article_dict)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    browser.quit()\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab862e5",
   "metadata": {},
   "source": [
    "### Connect to Reuters and Perform Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da2a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    keyword = 'israel hamas'\n",
    "    articles = reuters_search(keyword,\"advancedcustomeranalytics@gmail.com\",\"password\")\n",
    "    \n",
    "    csv_file_name = 'reuters_search_news_israel.csv'\n",
    "    \n",
    "    with open(csv_file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file, delimiter=';') \n",
    "        writer.writerow(['title', 'url', 'date', 'source', 'text'])\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                writer.writerow([\n",
    "                    article.get('title', ''),\n",
    "                    article.get('url', ''),\n",
    "                    article.get('date', ''),\n",
    "                    article.get('source', ''),\n",
    "                    article.get('text', '').replace('\\n', ' ').replace('\\r', '')  \n",
    "                ])\n",
    "            except:\n",
    "                pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
